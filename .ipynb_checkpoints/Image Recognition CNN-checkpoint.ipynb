{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539b6600-f291-44d3-8f65-ab4536628639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d163262c-fc36-42df-bdc9-274829b1cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Conv2D , MaxPooling2D , Flatten\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfeb6d4-d1b0-47a1-9a97-9845c02ed3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae5064-f1da-4b70-9032-4ae5a87a7efa",
   "metadata": {},
   "source": [
    "CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d415337-8f74-4c80-b102-12d9c36bedbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#keras.layers.Conv2D(filters,kernel_size,strides=(1, 1),padding=\"valid\",data_format=None,dilation_rate=(1, 1),groups=1,activation=None,use_bias=True,kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",\n",
    "#    kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,**kwargs)\n",
    "cnn.add(Conv2D(32,(3,3) , input_shape=(64,64,3) , activation = \"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Conv2D(16,(3,3), activation = \"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9869a7-986c-439b-94a2-c19b11342b1e",
   "metadata": {},
   "source": [
    "ANN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1891bc5-7216-4e76-ade9-2d099d172039",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(64,activation=\"relu\"))\n",
    "cnn.add(Dense(32,activation=\"relu\"))\n",
    "cnn.add(Dense(16,activation=\"relu\"))\n",
    "cnn.add(Dense(8,activation=\"relu\"))\n",
    "cnn.add(Dense(4,activation=\"relu\"))\n",
    "cnn.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62616688-f17a-4c92-950d-b7901392a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(loss=\"binary_crossentropy\" , optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc16dbd-aec1-4e41-8c9d-3354ddfdfb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_datagen = ImageDataGenerator(\\n        rescale=1./255,\\n        shear_range=0.2,\\n        zoom_range=0.2,\\n        horizontal_flip=True)\\n\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\n\\ntrain_generator = train_datagen.flow_from_directory(\\n        train_data_dir,\\n        target_size=(img_height, img_width),\\n        batch_size=batch_size,\\n        class_mode='binary')\\n\\nvalidation_generator = test_datagen.flow_from_directory(\\n        validation_data_dir,\\n        target_size=(img_height, img_width),\\n        batch_size=batch_size,\\n        class_mode='binary')\\n\\n# fine-tune the model\\nmodel.fit_generator(\\n        train_generator,\\n        steps_per_epoch=nb_train_samples // batch_size,\\n        epochs=epochs,\\n        validation_data=validation_generator,\\n        validation_steps=nb_validation_samples // batch_size)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample for the below\n",
    "'''train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2b1185-3e70-49eb-a0e2-f84d45f70cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 692ms/step - loss: 0.6943 - val_loss: 0.6824\n",
      "Epoch 2/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 610ms/step - loss: 0.6934 - val_loss: 0.6242\n",
      "Epoch 3/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 556ms/step - loss: 0.6897 - val_loss: 0.6165\n",
      "Epoch 4/5\n",
      "\u001b[1m182/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m4s\u001b[0m 275ms/step - loss: 0.6699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 519ms/step - loss: 0.6695 - val_loss: 0.9197\n",
      "Epoch 5/5\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 605ms/step - loss: 0.6439 - val_loss: 1.1126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2576ddffa10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\admin\\Desktop\\github\\dogcat\\train\",\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        r\"C:\\Users\\admin\\Desktop\\github\\dogcat\\test1\",\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "cnn.fit(train_generator,steps_per_epoch=200 ,epochs=5,validation_data=test_generator) #increase the size of steps_per_epoch and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7dfc9-524a-46a1-87cc-57c132c73e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "460dd0c0-54e7-47bf-8377-2efd1dcb8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb0bfa0-ee2e-4447-93d4-5047c96440e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(r\"C:\\Users\\admin\\Desktop\\github\\dogcat\\validation\\cats\\cat.3.jpg\" , target_size = (64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85441954-df3f-4fad-8022-e5280a2054e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[172., 163., 148.],\n",
       "        [194., 178., 165.],\n",
       "        [206., 191., 168.],\n",
       "        ...,\n",
       "        [195., 182., 166.],\n",
       "        [190., 177., 161.],\n",
       "        [184., 170., 157.]],\n",
       "\n",
       "       [[195., 187., 168.],\n",
       "        [206., 199., 180.],\n",
       "        [199., 193., 167.],\n",
       "        ...,\n",
       "        [187., 174., 158.],\n",
       "        [194., 181., 165.],\n",
       "        [187., 173., 160.]],\n",
       "\n",
       "       [[199., 185., 174.],\n",
       "        [183., 170., 161.],\n",
       "        [161., 144., 134.],\n",
       "        ...,\n",
       "        [190., 174., 158.],\n",
       "        [190., 174., 158.],\n",
       "        [194., 181., 164.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[167., 154., 138.],\n",
       "        [113., 101.,  79.],\n",
       "        [185., 172., 155.],\n",
       "        ...,\n",
       "        [173., 168., 149.],\n",
       "        [152., 147., 128.],\n",
       "        [176., 171., 149.]],\n",
       "\n",
       "       [[143., 131., 107.],\n",
       "        [154., 141., 124.],\n",
       "        [145., 139., 123.],\n",
       "        ...,\n",
       "        [121., 115.,  99.],\n",
       "        [208., 202., 186.],\n",
       "        [217., 209., 198.]],\n",
       "\n",
       "       [[191., 179., 155.],\n",
       "        [195., 182., 165.],\n",
       "        [181., 176., 156.],\n",
       "        ...,\n",
       "        [187., 181., 167.],\n",
       "        [189., 183., 167.],\n",
       "        [186., 184., 161.]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image.img_to_array(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa31675c-17bd-4a07-9a42-767c7435cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d3314ba-3e52-4414-bbe8-fab9efeaa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c89ed86-4a1e-4cca-b90a-56c37eaeb9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "prd=cnn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b003c7bc-3c6e-4728-af9a-5ca7003aad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "if prd[0][0]<0.5:\n",
    "    print(\"dog\")\n",
    "else:\n",
    "    print(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531da9e4-59bd-494a-b5ea-52bbd0209fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
